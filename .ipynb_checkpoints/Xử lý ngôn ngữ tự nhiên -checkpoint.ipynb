{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa75ae9",
   "metadata": {},
   "source": [
    "# I. Từ ghép và từ đơn trong tiếng việt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf64a49",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (1.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (3.7)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (6.0)\n",
      "Requirement already satisfied: unidecode in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (1.3.6)\n",
      "Requirement already satisfied: underthesea-core==0.0.5a2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (0.0.5a2)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (2.28.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (1.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (4.64.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea) (0.9.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from Click>=6.0->underthesea) (0.4.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from nltk->underthesea) (2022.10.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea) (2022.9.24)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->underthesea) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->underthesea) (1.23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->underthesea) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88048759",
   "metadata": {},
   "source": [
    "## 1. Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c343f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taylor cho biết lúc đầu cô cảm thấy ngại với cô bạn thân Amanda nhưng rồi mọi thứ trôi qua nhanh chóng.',\n",
       " 'Amanda cũng thoải mái với mối quan hệ này.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import sent_tokenize\n",
    "text = 'Taylor cho biết lúc đầu cô cảm thấy ngại với cô bạn thân Amanda nhưng rồi mọi thứ trôi qua nhanh chóng. Amanda cũng thoải mái với mối quan hệ này.'\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840d189",
   "metadata": {},
   "source": [
    "## 2. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4452628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Đảm bảo chất lượng phòng thí nghiệm hóa học'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import text_normalize\n",
    "text_normalize('Ðảm baỏ chất lựơng phòng thí nghịêm hoá học')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17fc4e",
   "metadata": {},
   "source": [
    "## 3. Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace6591d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hoa hồng',\n",
       " 'là',\n",
       " 'một',\n",
       " 'loại',\n",
       " 'hoa',\n",
       " 'được',\n",
       " 'mọi',\n",
       " 'người',\n",
       " 'yêu',\n",
       " 'thích']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "word_tokenize(sentence)\n",
    "word_tokenize(\"Hoa hồng là một loại hoa được mọi người yêu thích \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bfac88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence, format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac91689",
   "metadata": {},
   "source": [
    "## 4. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5f68c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chợ', 'N'),\n",
       " ('thịt', 'N'),\n",
       " ('chó', 'N'),\n",
       " ('nổi tiếng', 'A'),\n",
       " ('ở', 'E'),\n",
       " ('Sài Gòn', 'Np'),\n",
       " ('bị', 'V'),\n",
       " ('truy quét', 'V')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import pos_tag\n",
    "pos_tag('Chợ thịt chó nổi tiếng ở Sài Gòn bị truy quét')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814fc03",
   "metadata": {},
   "source": [
    "## 5. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b365502c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bác sĩ', 'N', 'B-NP'),\n",
       " ('bây giờ', 'P', 'B-NP'),\n",
       " ('có thể', 'R', 'O'),\n",
       " ('thản nhiên', 'A', 'B-AP'),\n",
       " ('báo', 'V', 'B-VP'),\n",
       " ('tin', 'N', 'B-NP'),\n",
       " ('bệnh nhân', 'N', 'B-NP'),\n",
       " ('bị', 'V', 'B-VP'),\n",
       " ('ung thư', 'N', 'B-NP'),\n",
       " ('?', 'CH', 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import chunk\n",
    "text = 'Bác sĩ bây giờ có thể thản nhiên báo tin bệnh nhân bị ung thư?'\n",
    "chunk(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e1b972",
   "metadata": {},
   "source": [
    "## 6. Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e439b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea[deep] in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (6.0)\n",
      "Requirement already satisfied: underthesea-core==0.0.5a2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (0.0.5a2)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (3.7)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (8.0.4)\n",
      "Requirement already satisfied: unidecode in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (1.3.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (1.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (2.28.1)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (0.9.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (1.1.2)\n",
      "Requirement already satisfied: torch<1.13,>=1.1.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (1.12.1)\n",
      "Requirement already satisfied: transformers>=3.5.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from underthesea[deep]) (4.24.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from Click>=6.0->underthesea[deep]) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from torch<1.13,>=1.1.0->underthesea[deep]) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (1.23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from transformers>=3.5.0->underthesea[deep]) (0.13.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea[deep]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea[deep]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea[deep]) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from requests->underthesea[deep]) (2022.9.24)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->underthesea[deep]) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from scikit-learn->underthesea[deep]) (3.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from packaging>=20.0->transformers>=3.5.0->underthesea[deep]) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea[deep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25262b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tối', 5, 'obl:tmod'),\n",
       " ('29/11', 1, 'compound'),\n",
       " (',', 4, 'punct'),\n",
       " ('Việt Nam', 5, 'nsubj'),\n",
       " ('thêm', 0, 'root'),\n",
       " ('2', 7, 'nummod'),\n",
       " ('ca', 5, 'obj'),\n",
       " ('mắc', 5, 'obj'),\n",
       " ('Covid-19', 5, 'punct')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import dependency_parse\n",
    "text = 'Tối 29/11, Việt Nam thêm 2 ca mắc Covid-19'\n",
    "dependency_parse(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0427abe",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "867c4d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chưa', 'R', 'O', 'O'),\n",
       " ('tiết lộ', 'V', 'B-VP', 'O'),\n",
       " ('lịch trình', 'V', 'B-VP', 'O'),\n",
       " ('tới', 'E', 'B-PP', 'O'),\n",
       " ('Việt Nam', 'Np', 'B-NP', 'B-LOC'),\n",
       " ('của', 'E', 'B-PP', 'O'),\n",
       " ('Tổng thống', 'N', 'B-NP', 'B-LOC'),\n",
       " ('Mỹ Donald Trump', 'Np', 'B-NP', 'I-LOC')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import ner\n",
    "text = 'Chưa tiết lộ lịch trình tới Việt Nam của Tổng thống Mỹ Donald Trump'\n",
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd4e90f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ner() got an unexpected keyword argument 'deep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munderthesea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ner\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBộ Công Thương xóa một tổng cục, giảm nhiều đầu mối\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ner() got an unexpected keyword argument 'deep'"
     ]
    }
   ],
   "source": [
    "from underthesea import ner\n",
    "text = \"Bộ Công Thương xóa một tổng cục, giảm nhiều đầu mối\"\n",
    "ner(text, deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e219a",
   "metadata": {},
   "source": [
    "## 8. Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2d1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d116142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the_thao']\n",
      "['kinh_doanh']\n",
      "['DISCOUNT']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ( classify('HLV đầu tiên ở Premier League bị sa thải sau 4 vòng đấu'))\n",
    "\n",
    "print ( classify('Hội đồng tư vấn kinh doanh Asean vinh danh giải thưởng quốc tế'))\n",
    "\n",
    "print ( classify('Lãi suất từ BIDV rất ưu đãi', domain='bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f036f43",
   "metadata": {},
   "source": [
    "## 9. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ecc7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n",
      "None\n",
      "['TRADEMARK#positive']\n"
     ]
    }
   ],
   "source": [
    "from underthesea import sentiment\n",
    "\n",
    "print(sentiment('hàng kém chất lg,chăn đắp lên dính lông lá khắp người. thất vọng'))\n",
    "print(sentiment('Sản phẩm hơi nhỏ so với tưởng tượng nhưng chất lượng tốt, đóng gói cẩn thận.'))\n",
    "\n",
    "print(sentiment('Đky qua đường link ở bài viết này từ thứ 6 mà giờ chưa thấy ai lhe hết', domain='bank'))\n",
    "print(sentiment('Xem lại vẫn thấy xúc động và tự hào về BIDV của mình', domain='bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3c341",
   "metadata": {},
   "source": [
    "# II. Tách tiếng việt không dấu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af861dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vistickedword in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (0.9.5)\n",
      "Requirement already satisfied: single-version<2.0,>=1.1 in c:\\users\\admin\\.conda\\envs\\tf_gpu\\lib\\site-packages (from vistickedword) (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install vistickedword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c60d70a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('thich', 'em', 'nhieu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vistickedword import split_words\n",
    "split_words(\"thichemnhieu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e2fbc",
   "metadata": {},
   "source": [
    "# III. Xóa icon + gộp từ + xóa stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b595741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi How is your and Have a nice weekend'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "def give_emoji_free_text(text): # xóa icon \n",
    "    allchars = [str for str in text.encode().decode('utf-8')]\n",
    "    emoji_list = [c for c in allchars if c in emoji.EMOJI_DATA]\n",
    "    clean_text = ' '.join([str for str in text.encode().decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "a = \"Hi 🤔 How is your 🙈 and 😌. Have a nice weekend 💕👭👙\"\n",
    "give_emoji_free_text(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0daa10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_synonyms(array_string ,array_words,keys,all_content = False): # gộp từ \n",
    "    if(all_content == False):\n",
    "        for ind in range(0,len(array_string)):\n",
    "            for words in array_words:\n",
    "                array_string[ind] = array_string[ind].replace(words,keys)\n",
    "    else:\n",
    "        for ind in range(0,len(array_string)):\n",
    "            for words in array_words:\n",
    "                if(array_string[ind] == words):\n",
    "                    array_string[ind] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd33d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_strings = ['ok' ,'Ok' ,'Hay','Sách hay' ,'hay' ,'In đẹp, đóng gói cẩn thận, giao hàng nhanh ' ,\n",
    "                 'Nội dung rất hay và ý nghĩa ạ🥰 ' ,'Giao hàng nhanh và đóng gói ok á',\n",
    "                 'Hơi nhỏ so với tưởng tượng đóng gói cũng không chắc chắn lắm',\n",
    "                 'Sách không bị lỗi form giữ cẩn thận . Sách đoc rất hay !!']\n",
    "combine_synonyms(array_strings,['Ok'],'ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cea499bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ok',\n",
       " 'ok',\n",
       " 'Hay',\n",
       " 'Sách hay',\n",
       " 'hay',\n",
       " 'In đẹp, đóng gói cẩn thận, giao hàng nhanh ',\n",
       " 'Nội dung rất hay và ý nghĩa ạ🥰 ',\n",
       " 'Giao hàng nhanh và đóng gói ok á',\n",
       " 'Hơi nhỏ so với tưởng tượng đóng gói cũng không chắc chắn lắm',\n",
       " 'Sách không bị lỗi form giữ cẩn thận . Sách đoc rất hay !!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e2b4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # hàm tạo ra stopword \n",
    "def get_stopwords(documents, threshold=3):\n",
    "    \"\"\"\n",
    "    :param documents: list of documents\n",
    "    :param threshold:\n",
    "    :return: list of words has idf <= threshold\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(min_df=100)\n",
    "    tfidf_matrix = tfidf.fit_transform(documents)\n",
    "    features = tfidf.get_feature_names()\n",
    "    stopwords = []\n",
    "    #print(min(tfidf.idf_), max(tfidf.idf_), len(features))\n",
    "    for index, feature in enumerate(features):\n",
    "        if tfidf.idf_[index] <= threshold:\n",
    "            stopwords.append(feature)\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae7b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35d4cc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a lô',\n",
       " 'a ha',\n",
       " 'ai',\n",
       " 'ai ai',\n",
       " 'ai nấy',\n",
       " 'ai đó',\n",
       " 'alô',\n",
       " 'amen',\n",
       " 'anh',\n",
       " 'anh ấy']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileObject = open(\"stopwords/vietnamese.txt\", \"r\",encoding=\"utf-8\")\n",
    "stopword_vn = fileObject.read().split(\"\\n\")\n",
    "stopword_vn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "175bb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea  \n",
    "def remove_stopwords(array_string,url_stopword = \"stopwords/vietnamese.txt\"):\n",
    "    arrays = []\n",
    "    fileObject = open(url_stopword, \"r\",encoding=\"utf-8\")\n",
    "    stopword_vn = fileObject.read().split(\"\\n\")\n",
    "    for i in range(0,len(array_string)):\n",
    "        word_tokens = underthesea.word_tokenize(array_string[i])\n",
    "        arrays.append(' '.join([w for w in word_tokens if not w.lower() in stopword_vn]) )\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2fecf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dòng sông', 'yêu vui vẻ', 'hạnh phúc']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_strings = [\"Ai là người đặt tên cho dòng sông\",\"Tôi yêu em nên tôi thấy rất vui vẻ\",\"anh ấy không phải là một người hạnh phúc\"]\n",
    "remove_stopwords(array_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bc14712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ai là người đặt tên cho dòng sông',\n",
       " 'Tôi yêu em nên tôi thấy rất vui vẻ',\n",
       " 'anh ấy không phải là một người hạnh phúc']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5c57dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>giao hàng nhanh, đóng gói cẩn thận, sách mới, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13871</th>\n",
       "      <td>Nội dung sách khá hay, nhưng sách cũ, nhiều vế...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13872</th>\n",
       "      <td>Sách không bị lỗi form giữ cẩn thận . Sách đoc...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>hài lòng</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13874</th>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  rating\n",
       "13870  giao hàng nhanh, đóng gói cẩn thận, sách mới, ...       5\n",
       "13871  Nội dung sách khá hay, nhưng sách cũ, nhiều vế...       1\n",
       "13872  Sách không bị lỗi form giữ cẩn thận . Sách đoc...       5\n",
       "13873                                           hài lòng       5\n",
       "13874                                               good       5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data.csv\").dropna()\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a032dc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mình k ở nhà, nhờ ngừoi nhà nhận hộ thì hộp như thế này đây, mở ra được luôn. Một bên bìa hộp thì rách đôi (có video nhưng ở đây k có mục up vid). Sách thì có vết như kia. Nhìn ảnh thì cũng bình thường nhưng thực tế nhìn ở bên ngoài và bản thân lại là người đi mua sách mới mà bị như vậy rất là khó chấp nhận. Mong Tiki xem xét lại khâu đóng gói hàng và nhân viên giao hàng.',\n",
       "       'Sách mới tinh, có màng bọc, giấy ok mà, giao hàng nhanh, đóng hộp chắc chắn. Mình mua lúc sale 40% nên lúc ý giá gần bằng giá bìa mềm nên mk đã chọn bìa cứng. Mặc dù giá bìa cứng bản của Mạnh Chương dịch rẻ hơn nhưng mk thấy bản của Nguyễn V Phước dịch hay hơn nên vẫn ch Thật may là k bị lỗi gì. Thật tuyệt vời. Nội dung thì k phải bàn cãi nữa, quá nổi tiếng rồi.',\n",
       "       'Đặt hàng 1/8/2021 Nhận hàng 25/9/2021 Dịch bệnh giản cách nên giao hàng chậm thì mình thông cảm. Nhưng nhận được thùng hàng đã bị rách đến mức \"hả họng\" ra hết. Sách là mặt hàng dễ bị ướt hư hỏng mà gói hàng rất cẩu thả, không chống nước và chồng sốc. May là lấy ra chỉ bị móp góc nhẹ. Mình tin dùng mà mua hàng Tiki cả trăm triệu rồi, nhưng đây là lần đầu mình complain về khâu đóng gói và kho hàng.',\n",
       "       ..., 'Sách không bị lỗi form giữ cẩn thận . Sách đoc rất hay !!',\n",
       "       'hài lòng', 'good'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.content.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfae1cf",
   "metadata": {},
   "source": [
    "# IV. Đếm số lần xuất hiện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "779fba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'người': 2,\n",
       " 'tôi': 2,\n",
       " 'là': 2,\n",
       " 'rất': 2,\n",
       " 'ai': 1,\n",
       " 'anh': 1,\n",
       " 'gặp': 1,\n",
       " 'được': 1,\n",
       " 'hân hạnh': 1,\n",
       " 'hạnh phúc': 1,\n",
       " 'một': 1,\n",
       " 'phải': 1,\n",
       " 'không': 1,\n",
       " 'ấy': 1,\n",
       " 'thấy': 1,\n",
       " 'vui vẻ': 1,\n",
       " 'nên': 1,\n",
       " 'em': 1,\n",
       " 'yêu': 1,\n",
       " 'sông': 1,\n",
       " 'dòng': 1,\n",
       " 'cho': 1,\n",
       " 'tên': 1,\n",
       " 'đặt': 1,\n",
       " 'bạn': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import underthesea  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "array_strings = [\"Ai là người đặt tên cho dòng sông\",\n",
    "                 \"Tôi yêu em nên tôi thấy rất vui vẻ\",\n",
    "                 \"anh ấy không phải là một người hạnh phúc\",\n",
    "                \"rất hân hạnh được gặp bạn\"]\n",
    "def word_count(array_string):\n",
    "    words = np.array([])\n",
    "    for i in array_string:\n",
    "        words = np.append(words,underthesea.word_tokenize(i.lower()))\n",
    "    X = pd.DataFrame(words,columns =['words'])\n",
    "    return X.words.value_counts()\n",
    "dict(word_count(array_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e0ce37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13870</th>\n",
       "      <td>giao hàng nhanh, đóng gói cẩn thận, sách mới, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13871</th>\n",
       "      <td>Nội dung sách khá hay, nhưng sách cũ, nhiều vế...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13872</th>\n",
       "      <td>Sách không bị lỗi form giữ cẩn thận . Sách đoc...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13873</th>\n",
       "      <td>hài lòng</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13874</th>\n",
       "      <td>good</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  rating\n",
       "13870  giao hàng nhanh, đóng gói cẩn thận, sách mới, ...       5\n",
       "13871  Nội dung sách khá hay, nhưng sách cũ, nhiều vế...       1\n",
       "13872  Sách không bị lỗi form giữ cẩn thận . Sách đoc...       5\n",
       "13873                                           hài lòng       5\n",
       "13874                                               good       5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e51f6037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11248</th>\n",
       "      <td>sản phẩm đóng gói ko cẩn thận , ko có gói chốn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6347</th>\n",
       "      <td>tạm được.................</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6341</th>\n",
       "      <td>chưa đọc sách ,mới nhận sách ,có quyển sách mà...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>Sản phẩm bị rách, cong; bookmark bị rách</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Đặt sách mới , giao sách cũ , chỉ có ở fahasa ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  rating\n",
       "11248  sản phẩm đóng gói ko cẩn thận , ko có gói chốn...       1\n",
       "6347                           tạm được.................       1\n",
       "6341   chưa đọc sách ,mới nhận sách ,có quyển sách mà...       1\n",
       "12321           Sản phẩm bị rách, cong; bookmark bị rách       1\n",
       "822    Đặt sách mới , giao sách cũ , chỉ có ở fahasa ...       1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sort_values(by=['rating']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78afc373",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data.rating.replace([[1,2,3],4,5] ,[\"không tốt\",\"bình thường\",\"tốt\"])\n",
    "data.drop(columns = \"rating\",inplace = True)\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcb3304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_count = dict(word_count(data.content.loc[data.target == \"tốt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bb2577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8087"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_count.get(\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e7b15",
   "metadata": {},
   "source": [
    "# V. Xóa dấu câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "efabddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi what is the weather like\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "a_string = '!hi. wh?at is the weat[h]er lik?e.'\n",
    "new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd425294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def dete_punctuation(array_string):\n",
    "    arrays = []\n",
    "    for i in array_string:\n",
    "        arrays.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "31b52af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Một ngày vui vẻ']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dete_punctuation([\"Một ngày vui vẻ.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d072a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dete_punctuation2(array_string,key='-'):\n",
    "    arrays = []\n",
    "    remove = string.punctuation.replace(key, \"\") \n",
    "    pattern = r\"[{}]\".format(remove) \n",
    "    for i in array_string:\n",
    "        arrays.append(re.sub(pattern, \"\", i))\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af3958df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this  is  @@a  test']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_strings  = [\")*^%{}[]thi's - is - @@#!a !%%!!%- test.\"]\n",
    "dete_punctuation2(array_strings,key ='@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
